# üÜì Ollama Setup Guide - Free & Open Source LLM

This guide will help you set up **Ollama** to use completely **FREE and OPEN SOURCE** language models with your RAG backend.

## üéØ **What is Ollama?**

Ollama is a free, open-source tool that allows you to run large language models **locally on your machine** without any API costs or usage limits.

## üöÄ **Quick Start (5 minutes)**

### 1. Install Ollama

**macOS:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download from [ollama.ai](https://ollama.ai/download)

### 2. Start Ollama Service
```bash
ollama serve
```

### 3. Download a Model
```bash
# Download Llama 2 (7B parameters - good balance of quality/speed)
ollama pull llama2

# Or try other free models:
ollama pull mistral      # Fast and efficient
ollama pull codellama    # Great for code
ollama pull phi2         # Lightweight and fast
```

### 4. Test Ollama
```bash
ollama run llama2 "Hello, how are you?"
```

## üîß **RAG Backend Integration**

### 1. Update Environment Variables
```bash
# In your .env file:
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

### 2. Start Your RAG Backend
```bash
python main.py
```

### 3. Test the Integration
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?"}'
```

## üìä **Available Free Models**

| Model | Size | Quality | Speed | Use Case |
|-------|------|---------|-------|----------|
| **llama2** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | General purpose |
| **mistral** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast & efficient |
| **codellama** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Code & technical |
| **phi2** | 2.7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Lightweight & fast |
| **llama2:13b** | 13B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | High quality |
| **mistral:7b-instruct** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Instruction following |

## üí° **Model Selection Guide**

- **For Development/Testing**: Use `phi2` (fastest, smallest)
- **For Production**: Use `llama2` or `mistral` (good balance)
- **For Code Documents**: Use `codellama`
- **For High Quality**: Use `llama2:13b` (slower but better)

## üîÑ **Switching Models**

### Via Environment Variable
```bash
# Change in .env file
OLLAMA_MODEL=phi3
```

### Via API (Runtime)
```python
from services.llm_service import LLMService

llm_service = LLMService()
llm_service.set_model("phi3")
```

## üì± **Performance Tips**

### 1. **Hardware Requirements**
- **Minimum**: 8GB RAM, 4GB free disk
- **Recommended**: 16GB+ RAM, 8GB+ free disk
- **Optimal**: 32GB+ RAM, dedicated GPU

### 2. **Speed Optimization**
```bash
# Use smaller models for faster responses
ollama pull phi2        # 2.7B parameters
ollama pull mistral     # 7B parameters

# Use larger models for better quality
ollama pull llama2:13b  # 13B parameters
```

### 3. **Memory Management**
```bash
# Check model memory usage
ollama list

# Remove unused models
ollama rm model_name
```

## üêõ **Troubleshooting**

### Common Issues

**1. Ollama not responding**
```bash
# Restart Ollama service
ollama serve

# Check if running
ps aux | grep ollama
```

**2. Model not found**
```bash
# List available models
ollama list

# Pull the model again
ollama pull llama2
```

**3. Out of memory**
```bash
# Use smaller model
ollama pull phi2

# Check system memory
free -h
```

**4. Slow responses**
```bash
# Use faster model
ollama pull phi2

# Check CPU usage
htop
```

## üîí **Security & Privacy**

‚úÖ **100% Private** - All processing happens on your machine  
‚úÖ **No Data Sharing** - No data sent to external services  
‚úÖ **No Usage Limits** - Unlimited queries  
‚úÖ **Offline Capable** - Works without internet after setup  

## üí∞ **Cost Comparison**

| Service | Cost per 1M tokens | Privacy | Speed |
|---------|-------------------|---------|-------|
| **Ollama (Local)** | $0.00 | 100% Private | Medium |
| **OpenAI GPT-3.5** | $0.002 | Data shared | Fast |
| **OpenAI GPT-4** | $0.03 | Data shared | Fast |
| **Anthropic Claude** | $0.015 | Data shared | Fast |

## üöÄ **Advanced Configuration**

### Custom Model Parameters
```python
llm_service.set_parameters(
    max_tokens=2000,
    temperature=0.5
)
```

## üìö **Additional Resources**

- [Ollama Documentation](https://ollama.ai/docs)
- [Model Library](https://ollama.ai/library)
- [Community Models](https://ollama.ai/library)
- [Performance Benchmarks](https://ollama.ai/benchmarks)

## üéâ **You're All Set!**

Your RAG backend now uses **completely free and open source** language models! 

- ‚úÖ **No API costs**
- ‚úÖ **No usage limits** 
- ‚úÖ **100% private**
- ‚úÖ **Offline capable**
- ‚úÖ **Enterprise ready**

Enjoy your free, powerful RAG system! üöÄ
